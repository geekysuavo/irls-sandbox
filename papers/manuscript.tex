\documentclass[final,5p,times,twocolumn]{elsarticle}
% ============================================================================

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lineno}
\usepackage{url}
\usepackage[
 pdftex,
 pdfauthor={Bradley Worley},
 pdftitle={Variationally reweighted least squares},
 hidelinks]{hyperref}

% ============================================================================
\newcommand{\m}[1]{\boldsymbol{#1}}
\newcommand{\E}[2]{\mathbb{E}_{#2}\left[ #1 \right]}
\newcommand{\entropy}[1]{\mathbb{H}\left[ #1 \right]}

\DeclareMathOperator{\trace}{tr}

% ============================================================================
\journal{Journal of Magnetic Resonance}

\begin{document}
\begin{frontmatter}

\title{Variationally reweighted least squares for sparse recovery}

\author{Bradley Worley\corref{cor1}}
\ead{geekysuavo@gmail.com}

%\author[addr1]{Bradley Worley\corref{cor1}}
%\ead{brad@atomwise.com}
%\cortext[cor1]{Corresponding author}
%\address[addr1]{Atomwise,
%                San Francisco, CA, 94133, USA}

% ============================================================================
\begin{abstract}
The task of recovering complete $n$D NMR spectra from nonuniformly
sampled (NUS) data has always fallen to optimization methods, the most
well-known of which are maximum entropy and iterative soft thresholding.
Both offer powerful optimality guarantees, but only yield point estimates
with little consideration given to the statistical nature of the problem.
This work introduces new methods of compressed sensing (CS) recovery that
directly treat spectral estimation as a Bayesian inference problem, which
is then converted to an optimization problem using variational techniques.
These methods are computationally efficient, yield posterior uncertainty
estimates, and possess many of the optimality guarantees of their
point-estimator analogues.
\end{abstract}

\begin{keyword}
Nonuniform sampling \sep
Compressed sensing \sep
Statistical inference \sep
Bayesian probability \sep
Variational methods
\end{keyword}

\end{frontmatter}
%% \linenumbers

% ============================================================================
\section{Introduction}
\label{s:intro}
Please fix me.

% ============================================================================
\section{Theory}
\label{s:theory}
The proposed variational algorithms strongly connect with a broad array
of existing results, making an initial exposition of all relevant theory
necessary.

We begin this section by introducing \emph{iteratively
reweighted least squares} (IRLS) as a classical method for solving sparse
recovery problems in compressed sensing. The machinery of Bayesian inference
is then brought to bear in order to demonstrate that IRLS naturally arises
as the \emph{maximum a posteriori} (MAP) estimator of a particular
probability distribution. Finally, the additional machinery of variational
approximation is employed to construct algorithms that are functionally
similar to IRLS but yield \emph{minimum mean-squared error} (MMSE)
estimates.

% ============================================================================
\subsection{Sparse recovery}
\label{ss:cs}
In compressed sensing, the concept of sparsity is leveraged to estimate
complete signals from incomplete measurements. In the context of NUS
NMR, the measured data vector $\m{y}$ is the result of applying the
measurement operator $\m{A}$ to an unknown spectrum $\m{x}$ and adding
noise corruptions $\m{\varepsilon}$, i.e.:
\begin{equation}
\m{y} = \m{A} \m{x} + \m{\varepsilon},
\label{eq:inv_prob}
\end{equation}
where $\m{A} = \m{B} \m{\Phi}$ is the product of the inverse discrete
Fourier transform (iDFT) matrix $\m{\Phi}$ and the subsampling matrix
$\m{B}$. Because $\m{A}$ has fewer rows than columns, we cannot solve
\eqref{eq:inv_prob} for $\m{x}$ without additional information.

To estimate $\m{x}$ despite the limited number of measurements, we include
our assumption that $\m{x}$ is \emph{sparse} (mostly zero) as an additional
source of information. The maximally sparse spectral estimate that agrees
exactly with the measured data is given by the solution to the following
problem:
\begin{equation}
\begin{aligned}
 \underset{\m{x}}{\text{minimize}} &\; \|\m{x}\|_0 \\
 \text{such that} &\; \m{y} = \m{A} \m{x}
\end{aligned}
\label{prob:l0}
\end{equation}
While conceptually appealing, this combinatorial problem is too
computationally expensive to be tractable for even modestly sized $\m{x}$.
Thus, we solve the convex relaxation of \eqref{prob:l0} instead:
\begin{equation}
\begin{aligned}
 \underset{\m{x}}{\text{minimize}} &\; \|\m{x}\|_1 \\
 \text{such that} &\; \m{y} = \m{A} \m{x}
\end{aligned}
\label{prob:l1}
\end{equation}
This problem may be efficiently solved by a variety of methods, including
\emph{iterative soft thresholding} (IST, \cite{stern:jmr2007}) and its
accelerated cousin NESTA \cite{becker:siam2011,sun:jbnmr2015}.

It is worthy of mention that the $\ell_1$ norm is hardly the only
sparsity-encoding function at our disposal. The Hoch-Hore entropy function
has a much longer history of use in NMR data processing \cite{hoch:jmr2017},
and has the added benefit of being smooth \cite{worley:jmr2016}. Non-convex
functions have also been used with NMR data \cite{kazimierczuk:jmr2011,
 worley:cmr2018}, but their use abrogates any existing guarantees that
the obtained solution is globally optimal. Among all sparsing-encoding
functions, the $\ell_1$ norm is uniquely qualified as the ``least convex''
convex function: it encodes sparsity more strongly than any other convex
function.

% ============================================================================
\subsection{Bias-variance tradeoffs}
\label{ss:bv_trade}
Because an unknown noise corruption $\m{\varepsilon}$ was added during the
measurement of $\m{y}$, enforcing exact agreement in problem \eqref{prob:l1}
is unreasonable, and effectively increases the variance of the estimate. As
is standard practice, we may assume that the noise corruptions are
independently and identically normally distributed (i.i.d.) with zero mean
and variance $\nu$, i.e.~$\varepsilon_i \sim \mathcal{N}(0, \nu)$, and that
$\nu$ is known exactly or to reasonable precision. In that case, we can
relax the data agreement constraint to yield an inequality-constrained
problem,
\begin{equation}
\begin{aligned}
 \underset{\m{x}}{\text{minimize}} &\; \|\m{x}\|_1 \\
 \text{such that} &\; \| \m{y} - \m{A} \m{x} \|_2^2 \le m \nu
\end{aligned}
\label{prob:l1_ic}
\end{equation}
where $m$ is the size of the data vector $\m{y}$. As $\nu$ is increased,
the estimate is biased \emph{away} from the measured data in favor of
decreasing the $\ell_1$ norm, thus reducing variance. In NUS NMR, this
bias-variance tradeoff is manifested as a ``linearity vs.~artifact''
tradeoff: low-bias methods that preserve linearity of spectral integrals
can more readily introduce spurious artifacts than low-variance methods
that sacrifice linearity in order to suppress artifacts.

We may also choose to remove the constraint entirely from problem
\eqref{prob:l1_ic} by expressing it as an unconstrained minimization
problem of the form,
\begin{equation}
\underset{\m{x}}{\text{minimize}} \left\{
 \|\m{x}\|_1 + \lambda \, \| \m{y} - \m{A} \m{x} \|_2^2
 \right\}
\label{prob:l1_uc}
\end{equation}
where $\lambda$ is a fixed parameter that balances the relative importance
of the $\ell_1$ and $\ell_2$ norms, and thus directly tunes the
bias-variance tradeoff. However, because $\lambda$ is generally unknown,
problems \eqref{prob:l1} and \eqref{prob:l1_ic} are more practically
useful than \eqref{prob:l1_uc}.

% ============================================================================
\subsection{IRLS}
\label{ss:irls}
The discontinuity of the $\ell_1$ norm at the origin raises challenges
when constructing minimization algorithms. Successful approaches rely
on incorporating smoothness. For example, IST and NESTA both introduce
strongly convex prox-functions into their objectives
\cite{beck:siam2009,becker:siam2011}.

Another natural approach to incorporating smoothness is by locally
approximating the $\ell_1$ norm by a quadratic function, i.e.:
\begin{equation}
\|\m{x}\|_1 =
 \sum_{i=1}^n |x_i| =
 \sum_{i=1}^n \frac{|x_i|^2}{|x_i|} \approx
 \sum_{i=1}^n \frac{|x_i|^2}{|\mu_i|} =
 \m{x}^* \m{W} \m{x}
\label{eq:l1_local_quad}
\end{equation}
where $\m{\mu}$ is the point around which the approximation is made,
$\m{W}$ is a diagonal matrix containing the weights $1 / |\mu_i|$,
and $\m{x}^*$ denotes the conjugate transpose of $\m{x}$. Substituting
this local approximation back into e.g.~\eqref{prob:l1} results in a
simple minimization algorithm composed of two steps per iteration:
\begin{enumerate}
 \item $\m{x} \gets \m{W}^{-1} \m{A}^*
        \left( \m{A} \m{W}^{-1} \m{A}^* \right)^{-1} \m{y}$
 \item $\forall i : W_{ii} \gets |x_i|^{-1}$
\end{enumerate}
which we shall refer to as \emph{equality-constrained iteratively
reweighted least squares}, or EC-IRLS. For details regarding the
derivation of EC-IRLS and all other algorithms discussed herein,
the reader is referred to the Supplementary Information.

One particularly serious flaw exists in the above algorithm: if any
$x_i \to 0$, as is often the case in sparse recovery, its accompanying
weight $W_{ii} \to \infty$. To avoid this, the weight update is modified
into $W_{ii} \gets (|x_i|^2 + \epsilon)^{-1/2}$, where $\epsilon > 0$ is a
small correction term \cite{daubechies:cpam2010}. For suitably chosen
$\epsilon$, the modified algorithm converges to the EC-IRLS solution.

% ============================================================================
\subsection{Bayesian inference}
\label{ss:bayes}
As many of the underlying quantities of interest in compressed sensing
recovery problems are either unknown or uncertain, it is reasonable to
consider these problems from a Bayesian perspective. In short, Bayesian
inference is a statistically consistent framework for updating beliefs
about uncertain quantities after observing new data \cite{ohagan2004},
and rests on an equation known as Bayes' rule:
\begin{equation}
p(\theta \mid y) =
 \frac{p(y \mid \theta) \, p(\theta)}{p(y)}
\label{eq:bayes_rule}
\end{equation}
which states that the probability of an unknown parameter $\theta$ given
data $y$, denoted $p(\theta \mid y)$, is proportional to the product of
two probabilities: that of observing $y$ given fixed $\theta$, denoted
$p(y \mid \theta)$, and that of $\theta$ alone, denoted $p(\theta)$.

We first define the likelihood of observing the data $\m{y}$ from some
fixed $\m{x}$. Assuming the measurement noise is i.i.d.~normal with
zero mean and variance $\nu$, we arrive at the following likelihood:
\begin{equation}
p(\m{y} \mid \m{x}) =
 (2 \pi \nu)^{-m/2} \exp\left\{
  -\frac{1}{2 \nu} \| \m{y} - \m{A} \m{x} \|_2^2
 \right\}
\label{eq:likelihood}
\end{equation}
To encode our belief that $\m{x}$ is sparse, we assign an i.i.d.~Laplace
prior to each $x_i$, which results in the following prior on $\m{x}$:
\begin{equation}
p(\m{x}) =
 (2 \xi)^{-n} \exp\left\{
  -\frac{1}{\xi} \| \m{x} \|_1
 \right\}
\label{eq:laplace_prior}
\end{equation}
In fact, our decision to use the Laplace prior is precisely because of
its connection to the $\ell_1$ norm. Applying Bayes' rule, we obtain
the posterior distribution of $\m{x}$ given $\m{y}$,
\begin{equation}
p(\m{x} \mid \m{y}) =
 Z_{x|y}^{-1} \exp\left\{
  -\frac{1}{2 \nu} \| \m{y} - \m{A} \m{x} \|_2^2
  -\frac{1}{\xi} \| \m{x} \|_1
 \right\}
\label{eq:laplace_post}
\end{equation}
where $Z_{x|y} = (2 \pi \nu)^{m/2} (2 \xi)^n p(\m{y})$. Maximizing
\eqref{eq:laplace_post} yields the MAP estimate, the most probable
value of $\m{x}$ given the data,
\begin{equation}
\begin{aligned}
\m{\hat x}_{\text{MAP}} &=
 \underset{\m{x}}{\arg\max} \; p(\m{x} \mid \m{y})
\\ &=
 \underset{\m{x}}{\arg\min} \left\{ -\ln p(\m{x} \mid \m{y}) \right\}
\\ &=
 \underset{\m{x}}{\arg\min} \left\{
  \frac{1}{2 \nu} \| \m{y} - \m{A} \m{x} \|_2^2 +
  \frac{1}{\xi} \| \m{x} \|_1
 \right\}
\end{aligned}
\label{prob:laplace_map}
\end{equation}
This problem is equivalent to the unconstrained compressed sensing
problem \eqref{prob:l1_uc}, with $\lambda = \xi / (2 \nu)$, thus
establishing a Bayesian framework for understanding sparse recovery.

By locally quadratically approximating the $\ell_1$ norm in problem
\eqref{prob:laplace_map}, we arrive at UC-IRLS, the unconstrained variant
of EC-IRLS. However, it will be more useful to construct IRLS without
relying on this quadratic approximation. In order to accomplish this,
we will replace the Laplace prior on $\m{x}$ with a \emph{hierarchical}
prior on $\m{x}$ and a new set of unknown variables $\m{\sigma}$:
\begin{equation}
p(\m{x}, \m{\sigma}) = p(\m{x} \mid \m{\sigma}) \, p(\m{\sigma})
\label{eq:hprior}
\end{equation}
Because each $(x_i, \sigma_i)$ is \emph{a priori} independent of the
others, $p(\m{x}, \m{\sigma}) = \prod_{i=1}^n p(x_i, \sigma_i)$, and
we can focus on the definintion of any single factor $p(x_i, \sigma_i)$.
We first assume that, conditioned on $\sigma_i$, $x_i$ is normally
distributed with zero mean and variance $\sigma_i^2$,
\begin{equation}
p(x_i \mid \sigma_i) =
 (2 \pi \sigma_i^2)^{-1/2} \exp\left\{
  -\frac{|x_i|^2}{2 \sigma_i^2}
 \right\}
\label{eq:hier_x}
\end{equation}
and then assign a Rayleigh prior distribution to $\sigma_i$:
\begin{equation}
p(\sigma_i) =
 \frac{\sigma_i}{\xi^2} \exp\left\{
  -\frac{\sigma_i^2}{2 \xi^2}
 \right\}
\label{eq:hier_sigma}
\end{equation}
By design, marginalizing this hierarchical prior over $\m{\sigma}$ recovers
a Laplace prior on $\m{x}$, i.e.:
\begin{equation}
\begin{aligned}
p(\m{x}) &=
 \int_0^\infty \cdots \int_0^\infty
 p(\m{x} \mid \m{\sigma}) \, p(\m{\sigma}) \,
 d\sigma_1 \cdots d\sigma_n
\\ &=
 (2 \xi)^{-n} \exp\left\{ -\frac{1}{\xi} \|\m{x}\|_1 \right\}
\end{aligned}
\label{eq:hier_marginal}
\end{equation}
As a consequence, we shall refer to this prior as the ``hierarchical-form''
Laplace prior, or $h\ell_1$ prior for short. Applying Bayes' rule once
again, we find the joint posterior of $\m{x}$ and $\m{\sigma}$ given
$\m{y}$,
\begin{equation}
p(\m{x}, \m{\sigma} \mid \m{y}) =
 Z_{x,\sigma|y}^{-1} \exp\left\{
  -\frac{1}{2 \nu} \| \m{y} - \m{A} \m{x} \|_2^2
  -\psi(\m{x}, \m{\sigma})
 \right\}
\label{eq:hier_post}
\end{equation}
where we have defined $\psi(\m{x}, \m{\sigma})$ as the negative logarithm
of the $h\ell_1$ prior (neglecting constant terms),
\begin{equation}
\psi(\m{x}, \m{\sigma}) =
 \frac{1}{2} \sum_{i=1}^n \left(
  \frac{|x_i|^2}{\sigma_i^2} +
  \frac{\sigma_i^2}{\xi^2}
 \right)
\label{eq:psi}
\end{equation}
As demonstrated in Supplementary Information, jointly maximizing
$p(\m{x}, \m{\sigma} \mid \m{y})$ by alternating between updates to
$\m{x}$ and $\m{\sigma}$ is equivalent to using UC-IRLS to maximize
$p(\m{x} \mid \m{y})$. With these results in hand, we are now sufficiently
prepared to introduce variational analogues of IRLS.

% ============================================================================
\subsection{Variational approximation}
\label{ss:vb}
At this point, we have established that IRLS obtains the MAP estimate of
$\m{x}$ for the posterior distributions \eqref{eq:laplace_post} and
\eqref{eq:hier_post}, but there are several reasons why we should
not settle for MAP. Most importantly, the maximizer of a probability
distribution is a \emph{local} property of that distribution that says
very little about the distribution as a whole. The power of Bayesian
inference lies in its ability to average over an ensemble of possible
models when forming estimates: MAP estimation is effectively ``throwing
the baby out with the bath water.''

A more useful Bayes estimator of $\m{x}$ is the posterior mean,
\begin{equation}
\m{\mu} =
 \E{\m{x}}{p(\m{x} \mid \m{y})} =
 \int_{\mathcal{X}} \m{x} \, p(\m{x} \mid \m{y}) \, d\m{x}
\label{eq:post_mean}
\end{equation}
or MMSE estimator, so called because it minimizes the expected
squared-error loss, i.e.:
\begin{equation}
\m{\mu} =
 \underset{\m{\theta}}{\arg\min} \;
 \E{\| \m{\theta} - \m{x} \|_2^2}{p(\m{x} \mid \m{y})}
\label{eq:mmse}
\end{equation}
Unfortunately, we cannot analytically compute the desired expectations
in \eqref{eq:post_mean}, making it necessary to resort to approximation
procedures if $\m{\mu}$ is desired.

One approximation procedure that has been effectively applied in the
statistical machine learning literature is \emph{variational inference}
\cite{bishop2007}, in which an approximate posterior
$q(\m{x}, \m{\sigma})$ is calibrated to the exact posterior
$p(\m{x}, \m{\sigma} \mid \m{y})$ by minimizing the
Kullback-Liebler divergence from $p$ to $q$:
\begin{equation}
\begin{aligned}
q^*(\m{x}, \m{\sigma}) &=
 \underset{q(\m{x}, \m{\sigma}) \in \mathcal{Q}}{\arg\min} \;
 \mathbb{D}_{\text{KL}}\big[
  q(\m{x}, \m{\sigma}) \,\big\|\, p(\m{x}, \m{\sigma} \mid \m{y})
 \big]
\\ &=
 \underset{q(\m{x}, \m{\sigma}) \in \mathcal{Q}}{\arg\min} \;
 \E{-\ln p(\m{x}, \m{\sigma}, \m{y})}{q(\m{x}, \m{\sigma})} -
 \entropy{q(\m{x}, \m{\sigma})}
\end{aligned}
\label{prob:var_inf}
\end{equation}
where $\mathcal{Q}$ is the set of all feasible distributions, referred to
as the \emph{variational family}, and $p(\m{x}, \m{\sigma}, \m{y})$ is
the joint distribution,
\begin{equation}
p(\m{x}, \m{\sigma}, \m{y}) =
 p(\m{y} \mid \m{x}) \,
 p(\m{x} \mid \m{\sigma}) \,
 p(\m{\sigma})
\label{eq:joint}
\end{equation}
Examination of the second form of problem \eqref{prob:var_inf} shows
that the variational problem involves the minimization of a free
energy, with the first term acting as an expected potential energy
and the second equaling the entropy of $q(\m{x}, \m{\sigma})$.
Furthermore, the expected potential is the \emph{convolution} of
the UC-IRLS objective function with $q(\m{x}, \m{\sigma})$, so
variational inference is minimizing a smoothed version of the
MAP objective. Finally, this smoothed objective is also convex,
as it is the convolution of a convex function with a non-negative
function.

At this point, many choices exist for how to proceed, so it will become
exceedingly important to carefully define terms. First, we may choose to
approximate $p(\m{x}, \m{\sigma} \mid \m{y})$ by inferring statistics
for $\m{x}$ and $\m{\sigma}$, or we may choose to approximate
$p(\m{z}, \m{\sigma} \mid \m{y})$ where $\m{z}$ is the iDFT of $\m{x}$,
i.e.~$\m{z} = \m{\Phi} \m{x}$. We will refer to the former class of
methods as \emph{direct}, and the latter class as \emph{indirect}.
With IRLS, we may easily obtain $\m{z}_{\text{MAP}}$ by first solving
for $\m{x}_{\text{MAP}}$ and Fourier-transforming, but the same will
not hold true for certain types of variational methods.

Our next modeling choice is in the variational family $\mathcal{Q}$. In
fact, we need only specify how $q(\m{x}, \m{\sigma})$ \emph{factorizes}
to construct a tractable variational inference algorithm that minimizes
\eqref{prob:var_inf}. Specifying how $q$ factorizes is tantamount to
making assumptions about which variables are correlated
\emph{a posteriori}. For example, mean-field (MF) families factorize
over all unknowns,
\begin{equation}
q(\m{x}, \m{\sigma}) =
 {\textstyle\prod}_{i=1}^n q(x_i) \, q(\sigma_i)
\label{eq:example_mf}
\end{equation}
which neglects all posterior correlations in $p$. This choice will lead
to very efficient algorithms at the cost of accuracy. If a closer
approximation to $p$ is needed, structured mean-field (SMF) families
of the form,
\begin{equation}
q(\m{x}, \m{\sigma}) = q(\m{x}) \, q(\m{\sigma})
\label{eq:example_smf}
\end{equation}
may be employed. These families capture additional posterior correlations
at the expense of substantially increased computation.

Finally, we may choose to move from unconstrained optimization of
\eqref{prob:var_inf} to a constrained optimization problem. In IRLS,
moving the data agreement term into a constraint was an effective means
of side-stepping our lack of knowledge of the correct value of $\xi$.
In VRLS, the value of $\xi$ remains important, even when constraints
are employed. We may begin to understand this by examining the variational
analogue to \eqref{eq:psi}:
\begin{equation}
\Psi(\m{\mu}, \m{\gamma}, \m{\alpha}) =
 \frac{1}{2 \xi} \sum_{i=1}^n \left(
  \frac{|\mu_i|^2 + \gamma_i}{\sqrt{\alpha_i}} + \sqrt{\alpha_i}
 \right)
 -\frac{1}{2} \sum_{i=1}^n \ln \gamma_i
\label{eq:psibar}
\end{equation}
which results from taking the expectation of $\psi(\m{x}, \m{\sigma})$
with $\m{x}$ normally distributed and $\m{\sigma}$ $h\ell_1$-distributed.
Here, $\m{\mu}$ and $\m{\gamma}$ are the mean and marginal variance of
$q(\m{x})$, and $\m{\alpha}$ is the weight parameter of $q(\m{\sigma})$.
These choices of distributions were not made arbitrarily, but were
instead determined through the calculus of variations. A detailed
description of this process is given in the Supplementary Information.

% ============================================================================
\section{Materials and methods}
\label{s:methods}
Please fix me.

% ============================================================================
\section{Results and discussion}
\label{s:results}
Please fix me.

\begin{itemize}
 \item Baked-in ``$\epsilon$-correction'' related to the variance
  estimate, which provides a natural interpretation of Daubechies'
  $\epsilon$ as a smoothing kernel width.
 \item The parameter $\xi$ tunes a different kind of bias-variance
  tradeoff: the one in the space of distributions $q(\m{x}, \m{\sigma})$.
  As $\xi \to 0$, $\gamma_i \to 0$ and we obtain ``MAP-like'' solutions.
  Conversely, as $\xi \to \infty$, $\gamma_i$ increases and we lose
  sparsity.
 \item Note the presence of $\ln \gamma_i$ terms in equation
  \eqref{eq:psibar}.
These terms come from the entropy $\entropy{q(\m{x})}$. If the entropy
terms were not present, we could trivially minimize $\Psi$ with respect
to $\m{\gamma}$ by setting $\m{\gamma}$ to zero.
 \item Prove convergence of VRLS, maybe rate too?
 \item From unconstrained to constrained: explore the severity of
  the bias-variance tradeoff in the variational framework.
 \item From structured mean-field to mean-field: explore the
  differences in results after neglecting posterior covariance,
  derive the serial and parallel mean-field updates.
 \item From time-domain to frequency domain: explore the differences
  in inferences from mean-field methods.
 \item Discuss computational complexity of all methods, and
  dispell the myth that IRLS is matrix-bound.
\end{itemize}

% ============================================================================
\section{Conclusions}
\label{s:concl}
Please fix me.

% ============================================================================
\appendix
\section{Supplementary information}
\label{a:si}
This material is available free of charge via the Internet at
\url{http://www.sciencedirect.com}. Supplementary data associated
with this article can be found, in the online version, at
\url{http://dx.doi.org/10.1016/j.jmr.0000.00.000}

% ============================================================================
\section*{References}
\bibliographystyle{elsarticle-num} 
\bibliography{vrls}

\end{document}
\endinput
