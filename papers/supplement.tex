
% ============================================================================
\documentclass{article}

% ============================================================================
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[
 top=1in,
 bottom=1.5in,
 inner=1.5in,
 outer=1.5in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{parskip}
\usepackage{color}
\usepackage{soul}

% ============================================================================
\newcommand{\m}[1]{\boldsymbol{#1}}
\newcommand{\E}[2]{\mathbb{E}_{#2}\left[ #1 \right]}
\newcommand{\entropy}[1]{\mathbb{H}\left[ #1 \right]}

\DeclareMathOperator{\trace}{tr}

% ============================================================================
\begin{document}

\author{Bradley Worley}
\title{Supplementary information for: \\
\emph{\Large Variationally reweighted least squares for sparse recovery}}

\maketitle

% ============================================================================
\section{IRLS}
\label{s:irls}
The EC-IRLS and UC-IRLS algorithms follow from minimizing the function,
\begin{equation}
\psi(\m{x}, \m{\sigma}) =
 \frac{1}{2} \sum_{i=1}^n \left(
  \frac{|x_i|^2}{\sigma_i^2} + \frac{\sigma_i^2}{\xi^2}
 \right)
\label{eq:psi}
\end{equation}
with and without data agreement constraints on $\m{x}$, respectively.
Iterations consist of alternating $\m{x}$ and $\m{\sigma}$ updates.
The iterates are initialized to $\m{x}^{(0)} = \m{0}$ and
$\m{\sigma}^{(0)} = \m{1}$. To ease notation, define
the (diagonal) IRLS weight matrix $\m{W}$ such that,
\begin{equation}
W_{ij} = \begin{cases}
 1 / \sigma_i^2 &\text{if } i = j \\
 0 &\text{if } i \ne j
\end{cases}
\label{eq:wmatrix}
\end{equation}

% ----------------------------------------------------------------------------
\subsection{Equality-constrained IRLS}
\subsubsection{Updates to $\m{x}$}
The EC-IRLS $\m{x}$-update is given by the sub-problem,
\begin{equation}
\m{x}^{(t+1)} =
\left\{
\begin{aligned}
 \underset{\m{x}}{\arg\min} &\;
 \psi\big( \m{x}, \m{\sigma}^{(t)} \big)
\\
 \text{such that} &\; \m{y} = \m{A} \m{x}
\end{aligned}
\right.
\label{prob:ec_irls_x}
\end{equation}

Introduce the Lagrangian for \eqref{prob:ec_irls_x},
\begin{equation}
\begin{aligned}
\mathcal{L}(\m{x}, \m{\lambda}) &=
 \psi\big( \m{x}, \m{\sigma}^{(t)} \big) +
 \m{\lambda}^* ( \m{y} - \m{A} \m{x} )
\\ &=
 \frac{1}{2} \m{x}^* \m{W}^{(t)} \m{x} +
 \m{\lambda}^* ( \m{y} - \m{A} \m{x} )
\end{aligned}
\label{eq:lagrangian_irls_x}
\end{equation}
where terms that do not involve $\m{x}$ have been dropped, because
they contribute a constant offset to the Lagrangian. Setting the
$\m{x}$-gradient of $\mathcal{L}(\m{x}, \m{\lambda})$ equal to
zero yields,
\begin{equation}
\begin{aligned}
\nabla_{\m{x}} \mathcal{L}(\m{x}, \m{\lambda}) &=
 \m{W}^{(t)} \m{x} - \m{A}^* \m{\lambda} = \m{0}
\\ \implies\quad
\m{x} &= \big(\m{W}^{(t)}\big)^{-1} \m{A}^* \m{\lambda}
\end{aligned}
\label{eq:ec_irls_xl}
\end{equation}

Substituting this back into the Lagrangian yields the dual function,
\begin{equation}
\begin{aligned}
g(\m{\lambda}) &\triangleq
 \underset{\m{x}}{\inf} \;\mathcal{L}(\m{x}, \m{\lambda})
\\ &=
 \frac{1}{2} \m{\lambda}^* \m{A}
 \big(\m{W}^{(t)}\big)^{-1} \m{W}^{(t)}
 \big(\m{W}^{(t)}\big)^{-1} \m{A}^* \m{\lambda}
 +\m{\lambda}^* \m{y}
 -\m{\lambda}^* \m{A} \big(\m{W}^{(t)}\big)^{-1} \m{A}^* \m{\lambda}
\\ &=
 -\frac{1}{2} \m{\lambda}^* \m{A}
  \big(\m{W}^{(t)}\big)^{-1}
  \m{A}^* \m{\lambda}
 +\m{\lambda}^* \m{y}
\end{aligned}
\label{eq:ec_irls_dual}
\end{equation}

Maximizing this dual function by setting its $\m{\lambda}$-gradient
to zero yields,
\begin{equation}
\begin{aligned}
\nabla_{\m{\lambda}} g(\m{\lambda}) &=
 -\m{A} \big(\m{W}^{(t)}\big)^{-1} \m{A}^* \m{\lambda}
 +\m{y} = \m{0}
\\ \implies \quad
\m{\lambda} &=
 \left( \m{A} \big(\m{W}^{(t)}\big)^{-1} \m{A}^* \right)^{-1} \m{y}
\end{aligned}
\label{eq:ec_irls_lambda}
\end{equation}

Thus, the solution to sub-problem \eqref{prob:ec_irls_x} is,
\begin{equation}
\m{x}^{(t+1)} =
 \big(\m{W}^{(t)}\big)^{-1} \m{A}^*
 \left( \m{A} \big(\m{W}^{(t)}\big)^{-1} \m{A}^* \right)^{-1} \m{y}
\label{eq:ec_irls_x}
\end{equation}

% ----------------------------------------------------------------------------
\subsubsection{Updates to $\m{\sigma}$}
The EC-IRLS $\m{\sigma}$-update is given by the sub-problem,
\begin{equation}
\m{\sigma}^{(t+1)} =
 \underset{\m{\sigma}}{\arg\min} \;
 \psi\big( \m{x}^{(t+1)}, \m{\sigma} \big)
\label{prob:ec_irls_sigma}
\end{equation}

Because $\psi(\m{x}, \m{\sigma})$ is separable with respect to each
$\sigma_i$, this involves the solution of $n$ independent sub-problems:
\begin{equation}
\sigma_i^{(t+1)} =
 \underset{\sigma_i}{\arg\min} \left(
  \frac{\big| x_i^{(t+1)} \big|^2}{2 \sigma_i^2} +
  \frac{\sigma_i^2}{2 \xi^2}
 \right)
\label{prob:ec_irls_sigma_i}
\end{equation}

Setting the gradient of this function equal to zero yields the
solution,
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \sigma_i} \left(
  \frac{\big| x_i^{(t+1)} \big|^2}{2 \sigma_i^2} +
  \frac{\sigma_i^2}{2 \xi^2}
 \right) &=
-\frac{\big| x_i^{(t+1)} \big|^2}{\sigma_i^3} +
 \frac{\sigma_i}{\xi^2} = 0
\\ \implies \quad
\sigma_i^{(t+1)} &= \sqrt{\xi \big| x_i^{(t+1)} \big|}
\end{aligned}
\label{eq:ec_irls_sigma_i}
\end{equation}
which is equivalent (by the definition of $\m{W}$) to the following
weight update:
\begin{equation}
W_{ii}^{(t+1)} =
 \frac{1}{\big( \sigma_i^{(t+1)} \big)^2} =
 \frac{1}{\xi \big| x_i^{(t+1)} \big|}
\label{eq:ec_irls_w}
\end{equation}

% ----------------------------------------------------------------------------
\subsection{Unconstrained IRLS}
In UC-IRLS, the exact constraints on $\m{x}$ are replaced with an
$\ell_2$-norm term, resulting in a new objective function,
\begin{equation}
\psi_\nu(\m{x}, \m{\sigma}) =
 \psi(\m{x}, \m{\sigma}) + \frac{1}{2 \nu} \| \m{y} - \m{A} \m{x} \|_2^2
\label{eq:uc_irls_obj}
\end{equation}

% ----------------------------------------------------------------------------
\subsubsection{Updates to $\m{x}$}
The UC-IRLS $\m{x}$-update is given by the sub-problem,
\begin{equation}
\m{x}^{(t+1)} =
 \underset{\m{x}}{\arg\min} \;
 \psi_\nu\big( \m{x}, \m{\sigma}^{(t)} \big)
\label{prob:uc_irls_x}
\end{equation}

Setting the $\m{x}$-gradient of
$\psi_\nu\big( \m{x}, \m{\sigma}^{(t)} \big)$
equal to zero yields the solution to sub-problem \eqref{prob:uc_irls_x},
\begin{equation}
\begin{aligned}
\nabla_{\m{x}} \psi_\nu\big( \m{x}, \m{\sigma}^{(t)} \big) &=
 \m{W}^{(t)} \m{x} + \nu^{-1} \m{A}^* \m{A} \m{x} -
 \nu^{-1} \m{A}^* \m{y} = \m{0}
\\ \implies \quad
\m{x}^{(t+1)} &=
 \nu^{-1} \left(
  \m{W}^{(t)} + \nu^{-1} \m{A}^* \m{A}
 \right)^{-1} \m{A}^* \m{y}
\end{aligned}
\label{eq:uc_irls_x}
\end{equation}

% ----------------------------------------------------------------------------
\subsubsection{Updates to $\m{\sigma}$}
Because $\psi_\nu(\m{x}, \m{\sigma})$ only depends on $\m{\sigma}$ through
$\psi(\m{x}, \m{\sigma})$, the UC-IRLS $\m{\sigma}$-update is identical
to the EC-IRLS update given by equation \eqref{eq:ec_irls_sigma_i}.

% ============================================================================
\section{Variational approximations}
\label{s:var_approx}
In variational inference, we approximate the exact posterior density
$p(\m{x}, \m{\sigma} \mid \m{y})$ with an approximating density
$q(\m{x}, \m{\sigma})$, which is restricted to a set of feasible
density functions called the variational family $\mathcal{Q}$.

Methods that infer the full time-domain signal $\m{z} = \m{\Phi} \m{x}$
employ approximating densities of the form $q(\m{z}, \m{\sigma})$. We
refer to these methods as \emph{indirect} methods, whereas those
inferring $\m{x}$ are \emph{direct}. Each method is defined by
its own variational family:
\begin{itemize}
 \item $\mathcal{Q}_{\text{SMF}}^{\m{x}}$: Direct, structured mean-field.
 \item $\mathcal{Q}_{\text{MF}}^{\m{x}}$: Direct, factorized mean-field.
 \item $\mathcal{Q}_{\text{SMF}}^{\m{z}}$: Indirect, structured mean-field.
 \item $\mathcal{Q}_{\text{MF}}^{\m{z}}$: Indirect, factorized mean-field.
\end{itemize}
Structured mean-field (SMF) models neglect posterior correlations between
$\m{x}$ (or $\m{z}$) and $\m{\sigma}$, while mean-field (MF) methods
neglect all posterior correlations, i.e.:
\begin{equation}
\begin{aligned}
\mathcal{Q}_{\text{SMF}}^{\m{x}} &= \big\{
  q \in \mathcal{P}
 \;\big|\;
  q(\m{x}, \m{\sigma}) = q(\m{x}) \, q(\m{\sigma})
 \big\}
\\
\mathcal{Q}_{\text{MF}}^{\m{x}} &= \big\{
  q \in \mathcal{P}
 \;\big|\;
  q(\m{x}, \m{\sigma}) = {\textstyle\prod}_{i=1}^n q(x_i) \, q(\sigma_i)
 \big\}
\end{aligned}
\label{eq:smf_mf}
\end{equation}
where $\mathcal{P}$ denotes the set of \emph{all} probability distributions
over $(\m{x}, \m{\sigma})$.

Given a certain variational family, each factor of $q(\m{x}, \m{\sigma})$
is updated as follows:
\begin{enumerate}
 \item Take the expectation of $\ln p(\m{x}, \m{\sigma}, \m{y})$ with
  respect to all factors other than the factor being updated.
 \item Exponentiate the resulting expectation and renormalize.
\end{enumerate}
This process is referred to as ``free-form'' variational updating, as the
form of the updated factor was determined by the calculus of variations,
not by the specification of $\mathcal{Q}$. The remainder of this section
demonstrates the results of free-form updates of each method.

% ----------------------------------------------------------------------------
\subsection{Direct methods}
\label{ss:var_direct}

\subsubsection{SMF $q(\m{x})$ updates}
In practice, the form of the updated factor is determined by examining
it in logarithmic form, i.e.:
\begin{equation}
\begin{aligned}
q^*(\m{x}) &= \frac{
 \exp\left(
  \E{\ln p(\m{x}, \m{\sigma}, \m{y})}{q(\m{\sigma})}
 \right)}
{\int_{\mathcal{X}} \exp\left(
  \E{\ln p(\m{x}, \m{\sigma}, \m{y})}{q(\m{\sigma})}
 \right) d\m{x}}
\\ \equiv \quad
\ln q^*(\m{x}) &=
 \E{\ln p(\m{x}, \m{\sigma}, \m{y})}{q(\m{\sigma})} + \text{const.}
\end{aligned}
\label{eq:direct_smf_x}
\end{equation}
which in this case is given by,
\begin{equation*}
\begin{aligned}
\ln q^*(\m{x}) &=
 -\frac{1}{2 \nu} \| \m{y} - \m{A} \m{x} \|_2^2
 -\frac{1}{2} \m{x}^* \m{V} \m{x}
 +\text{const.}
\end{aligned}
\end{equation*}
where $\m{V} = \E{\m{W}}{q(\m{\sigma})}$ is the variational weight matrix.
This function contains linear and quadratic terms in $\m{x}$, i.e. $x_i$
and $x_i x_j$, and therefore matches a multivariate normal whose mean
and variance are given by (after completing the square):
\begin{equation*}
\begin{aligned}
\m{\mu} &= \nu^{-1} \m{\Gamma} \m{A}^* \m{y}
\\
\m{\Gamma} &=
 \m{P}^{-1} = \left( \nu^{-1} \m{A}^* \m{A} + \m{V} \right)^{-1}
\end{aligned}
\end{equation*}
For expediency, we will use $\m{\gamma}$ to denote the diagonal elements
of $\m{\Gamma}$.

\subsubsection{Serial MF $q(x_i)$ updates}
In factorized mean-field models, the form of the updated factor over
any $q(x_i)$ is given by,
\begin{equation*}
\ln q^*(x_i) =
 -\frac{1}{2} P_{ii} |x_i|^2
 -\bar{x}_i \sum_{j \ne i} P_{ij} \mu_j
 +\bar{x}_i \nu^{-1} \left[ \m{A}^* \m{y} \right]_i
 +\text{const.}
\end{equation*}
which is a normal distribution with the following mean and variance:
\begin{equation*}
\begin{aligned}
\mu_i &=
 P_{ii}^{-1} \left(
  \nu^{-1} \left[ \m{A}^* \m{y} \right]_i -
  {\textstyle\sum}_{j \ne i} P_{ij} \mu_j
 \right)
\\
\gamma_i &= P_{ii}^{-1}
\end{aligned}
\end{equation*}

\subsubsection{Parallel MF $q(x_i)$ updates}
We may rewrite $\ln q^*(x_i)$ in a way that exposes the matrix-vector
product $\m{P} \m{\mu}$ more clearly,
\begin{equation*}
\ln q^*(x_i) =
 -\frac{1}{2} P_{ii} |x_i|^2
 +\bar{x}_i \left(
   \left[ \nu^{-1} \m{A}^* \m{y} - \m{P} \m{\mu} \right]_i
     + P_{ii} \mu_i
   \right)
 +\text{const.}
\end{equation*}
resulting in an equivalent update rule:
\begin{equation*}
\begin{aligned}
\mu_i &\gets \mu_i +
 P_{ii}^{-1} \left[ \nu^{-1} \m{A}^* \m{y} - \m{P} \m{\mu} \right]_i
\\
\gamma_i &\gets P_{ii}^{-1}
\end{aligned}
\end{equation*}
where the assignment symbol ``$\gets$'' has been used instead of ``$=$''
to emphasize the difference between right-hand and left-hand $\m{\mu}$.
This suggests the possibility of updating \emph{all} $q(x_i)$ factors
in parallel with the following steps:
\begin{enumerate}
 \item $\forall i : \gamma_i \gets P_{ii}^{-1}$
 \item $\m{\mu} \gets \m{\mu} + \m{\gamma} \circ \left(
         \nu^{-1} \m{A}^* \m{y} - \m{P} \m{\mu}
        \right)$
\begin{equation*}
\end{equation*}
\end{enumerate}
where ``$\circ$'' denotes the element-wise product. This update is
exceedingly cheap when $\m{\Phi}$ is the iDFT matrix, because multiplying
$\m{\mu}$ by $\m{P}$ involves two fast Fourier transforms.

\subsubsection{$q(\m{\sigma})$ updates}
In both SMF and MF updates, the function $\ln q^*(\m{\sigma})$ is separable
with respect to each $\sigma_i$, meaning that after exponentiation and
renormalization, $q^*(\m{\sigma})$ will be a product of $n$ independent
factors over each $\sigma_i$. Therefore, we examine the form of a
single factor:
\begin{equation*}
\ln q^*(\sigma_i) =
 -\frac{\E{|x_i|^2}{q(\m{x})}}{2 \sigma_i^2}
 -\frac{\sigma_i^2}{2 \xi^2}
 +\text{const.}
\end{equation*}
which is precisely that of the $h\ell_1$ density introduced in section
\ref{s:hprior}, with parameters:
\begin{equation*}
\begin{aligned}
\alpha_i &= \E{|x_i|^2}{q(\m{x})} = |\mu_i|^2 + \gamma_i
\\
\beta_i &= \xi
\end{aligned}
\end{equation*}

% ----------------------------------------------------------------------------
\subsection{Indirect methods}
\label{ss:var_indirect}
Indirect methods may be formally obtained by first making a change of
variables in the joint distribution:
\begin{equation}
p(\m{z}, \m{\sigma}, \m{y}) = \int_{\mathcal{X}}
 p(\m{x}, \m{\sigma}, \m{y}) \, \delta(\m{z} - \m{\Phi} \m{x}) \, d\m{x}
\label{eq:x_to_z}
\end{equation}
which amounts to replacing each instance of $\m{\Phi} \m{x}$ with $\m{z}$,
and each instance of $\m{x}$ with $\m{\Phi}^{-1} \m{z}$ in the distribution.
When $\m{\Phi}$ is unitary, as is the case for the iDFT, we use
$\m{x} = \m{\Phi}^* \m{z}$.

\subsubsection{SMF $q(\m{z})$ updates}
The updated SMF factor over $\m{z}$ has the logarithmic form,
\begin{equation*}
\ln q^*(\m{z}) =
 -\frac{1}{2 \nu} \| \m{y} - \m{B} \m{z} \|_2^2
 -\frac{1}{2} \m{z}^* \m{\Phi} \m{V} \m{\Phi}^* \m{z}
 +\text{const.}
\end{equation*}
which implies $q^*(\m{z})$ is a multivariate normal distribution
with mean and variance,
\begin{equation*}
\begin{aligned}
\m{\tilde\mu} &= \nu^{-1} \m{\tilde\Gamma} \m{B}^\top \m{y}
\\
\m{\tilde\Gamma} &=
 \m{\tilde P}^{-1} = \left(
  \nu^{-1} \m{B}^\top \m{B} + \m{\Phi} \m{V} \m{\Phi}^*
 \right)^{-1}
\end{aligned}
\end{equation*}

\subsubsection{Serial MF $q(z_i)$ updates}
The updated MF factor over each $z_i$ has the logarithmic form,
\begin{equation*}
\ln q^*(z_i) =
 -\frac{1}{2} \tilde{P}_{ii} |z_i|^2
 -\bar{z}_i \sum_{j \ne i} \tilde{P}_{ij} \tilde\mu_j
 +\bar{z}_i \nu^{-1} \left[ \m{B}^\top \m{y} \right]_i
 +\text{const.}
\end{equation*}
implying $q^*(z_i)$ is a normal distribution with mean and variance,
\begin{equation*}
\begin{aligned}
\tilde\mu_i &=
 \tilde{P}_{ii}^{-1} \left(
  \nu^{-1} \left[ \m{B}^\top \m{y} \right]_i -
  {\textstyle\sum}_{j \ne i} \tilde{P}_{ij} \tilde\mu_j
 \right)
\\
\tilde\gamma_i &= \tilde{P}_{ii}^{-1}
\end{aligned}
\end{equation*}

\subsubsection{Parallel MF $q(z_i)$ updates}
Tracing the same logic as above, we obtain the following parallel update
rule for $q^*(\m{z})$:
\begin{enumerate}
 \item $\forall i : \tilde\gamma_i \gets \tilde{P}_{ii}^{-1}$
 \item $\m{\tilde\mu} \gets \m{\tilde\mu} + \m{\tilde\gamma} \circ \left(
         \nu^{-1} \m{B}^\top \m{y} - \m{\tilde P} \m{\tilde\mu}
        \right)$
\begin{equation*}
\end{equation*}
\end{enumerate}

\subsubsection{$q(\m{\sigma})$ updates}
In indirect methods, $q^*(\m{\sigma})$ still has the same form as its
direct counterpart, but the computation of each $\alpha_i$ is
subtly changed:
\begin{equation*}
\alpha_i = \E{|x_i|^2}{q(\m{z})} = |\mu_i|^2 + \gamma_i
\end{equation*}
These are the same $\m{\mu}$ and $\m{\gamma}$ as in the direct case, but
now they must be computed \emph{through} $q(\m{z})$. Following the rules
for linear transformation of normal random variates, we obtain the
necessary values:
\begin{equation*}
\begin{aligned}
\m{\mu} &= \m{\Phi}^* \m{\tilde\mu}
\\
\m{\Gamma} &= \m{\Phi} \m{\tilde\Gamma} \m{\Phi}^*
\end{aligned}
\end{equation*}
While $\m{\mu}$ may be cheaply computed from $\m{\tilde\mu}$, the same
cannot be said (in general) for $\m{\Gamma}$. Thankfully, when
$\m{\tilde\Gamma}$ is diagonal (i.e.~in MF), we may obtain the
diagonal elements of $\m{\Gamma}$ using a slightly cheaper
expression:
\begin{equation*}
\m{\gamma} = \left| \m{\Phi} \right|^2 \m{\tilde\gamma}
\end{equation*}
where the squared modulus $|\cdot|^2$ is applied element-wise to
$\m{\Phi}$. Even better, when $\m{\Phi}$ is the iDFT matrix, we
obtain the following simplification:
\begin{equation*}
\forall i : \gamma_i = \frac{1}{n} \sum_{i=1}^n \tilde\gamma_i
\end{equation*}

% ============================================================================
\section{Direct VRLS}
\label{s:vrls}
Direct VRLS algorithms involve minimizing the function,
\begin{equation}
\begin{aligned}
\Psi(\m{\mu}, \m{\gamma}, \m{\alpha}) &\triangleq
 \inf_{\m{\beta}} \left\{
  \E{\psi(\m{x}, \m{\sigma})}{q(\m{x}, \m{\sigma})} -
  \entropy{q(\m{\sigma})}
 \right\}
\\ &=
 \frac{1}{2 \xi} \sum_{i=1}^n \left(
  \frac{|\mu_i|^2 + \gamma_i}{\sqrt{\alpha_i}} + \sqrt{\alpha_i}
 \right)
\end{aligned}
\label{eq:psibar}
\end{equation}
with and without data agreement constraints on $\{\m{\mu}, \m{\gamma}\}$,
and in the presence of an entropy term $\entropy{q(\m{x})}$, where $q$
denotes the variational approximation of the posterior
$p(\m{x}, \m{\sigma} \mid \m{y})$. Similarly to IRLS, the iterates are
initialized to $\m{\mu}^{(0)} = \m{0}$, $\m{\gamma}^{(0)} = \m{1}$ and
$\m{\alpha}^{(0)} = \m{1}$. To ease notation once again, define
the (diagonal) VRLS weight matrix $\m{V}$ such that,
\begin{equation}
V_{ij} = \begin{cases}
 1/(\xi \sqrt{\alpha_i}) &\text{if } i = j \\
 0 &\text{if } i \ne j
\end{cases}
\label{eq:vmatrix}
\end{equation}

Note that $\Psi$ was defined such that $\m{\beta}$ is removed by
minimization. As seen from the free-form variational updates to
$q(\m{\sigma})$ above, $\m{\beta}$ is not a free parameter vector,
but instead has its value fixed to $\xi \m{1}$.

In what follows, the expectation of an $\ell_2$ norm with respect to
a multivariate normal distribution will frequently appear. We employ
the following identity:
\begin{equation}
\E{\| \m{y} - \m{A} \m{x} \|_2^2}
  {\mathcal{N}(\m{x}; \m{\mu}, \m{\Gamma})} =
\| \m{y} - \m{A} \m{\mu} \|_2^2 + \trace(\m{A}^* \m{A} \m{\Gamma})
\label{eq:l2expect}
\end{equation}
which applies generally. When $\m{\Gamma}$ is a diagonal matrix with
elements $\m{\gamma}$, the trace term simplifies into an inner product,
\begin{equation}
\trace(\m{A}^* \m{A} \m{\Gamma}) = \m{a}^\top \m{\gamma}
\label{eq:diag_trace}
\end{equation}
where $\m{a}$ is the (real) vector of diagonal elements of $\m{A}^* \m{A}$.
We will also employ $\m{b}$ as the vector of diagonal elements of
$\m{B}^\top \m{B}$.

% ----------------------------------------------------------------------------
\subsection{Unconstrained SMF}
Dir-SMF-UC-VRLS minimizes the objective,
\begin{equation}
\Psi_\nu(\m{\mu}, \m{\Gamma}, \m{\alpha}) =
 \Psi(\m{\mu}, \m{\gamma}, \m{\alpha})
 +\frac{1}{2 \nu} \| \m{y} - \m{A} \m{\mu} \|_2^2
 +\frac{1}{2 \nu} \trace(\m{A}^* \m{A} \m{\Gamma})
 -\frac{1}{2} \ln \det(\m{\Gamma})
\label{eq:uc_vrls_smf_obj}
\end{equation}

\subsubsection{$\m{\mu}$ updates}
\hl{FIXME}

\subsubsection{$\m{\Gamma}$ updates}
\hl{FIXME}

\subsubsection{$\m{\alpha}$ updates}
\hl{FIXME}

% ----------------------------------------------------------------------------
\subsection{Unconstrained MF}
Dir-MF-UC-VRLS minimizes the objective,
\begin{equation}
\Psi_\nu(\m{\mu}, \m{\gamma}, \m{\alpha}) =
 \Psi(\m{\mu}, \m{\gamma}, \m{\alpha})
 +\frac{1}{2 \nu} \| \m{y} - \m{A} \m{\mu} \|_2^2
 +\frac{1}{2 \nu} \m{a}^\top \m{\gamma}
 -\frac{1}{2} \sum_{i=1}^n \ln \gamma_i
\label{eq:uc_vrls_mf_obj}
\end{equation}

\subsubsection{$\m{\mu}$ updates}
\hl{FIXME}

\subsubsection{$\m{\gamma}$ updates}
\hl{FIXME}

\subsubsection{$\m{\alpha}$ updates}
\hl{FIXME}

% ----------------------------------------------------------------------------
\subsection{Equality-constrained MF}
\hl{FIXME}

\subsubsection{$\m{\mu}$ updates}
\hl{FIXME}

\subsubsection{$\m{\gamma}$ updates}
\hl{FIXME}

\subsubsection{$\m{\alpha}$ updates}
\hl{FIXME}

% ============================================================================
\section{The $h\ell_1$ density}
\label{s:hprior}
The ``hierarchical-form Laplace'' probability density function,
or $h\ell_1$ density for short, has the following form:
\begin{equation}
h(\sigma; \alpha, \beta) =
 \frac{1}{Z(\alpha, \beta)}
 \exp\left\{
  -\frac{\alpha}{2 \sigma^2}
  -\frac{\sigma^2}{2 \beta^2}
 \right\}
\label{eq:hprior}
\end{equation}
where $\alpha, \beta, \sigma > 0$ and $Z(\alpha, \beta)$ is the
normalization function, which is defined to be,
\begin{equation}
\begin{aligned}
Z(\alpha, \beta) &\triangleq
 \int_0^\infty \exp\left\{
  -\frac{\alpha}{2 \sigma^2}
  -\frac{\sigma^2}{2 \beta^2}
 \right\} d\sigma
\\ &=
 \beta \sqrt{\frac{\pi}{2}}
 \exp\left\{ -\frac{\sqrt{\alpha}}{\beta} \right\}
\end{aligned}
\label{eq:hprior_z}
\end{equation}

Thus, the fully normalized $h\ell_1$ density is equal to,
\begin{equation}
h(\sigma; \alpha, \beta) =
 \frac{1}{\beta} \sqrt{\frac{2}{\pi}}
 \exp\left\{
  -\frac{\alpha}{2 \sigma^2}
  -\frac{\sigma^2}{2 \beta^2}
  +\frac{\sqrt{\alpha}}{\beta}
 \right\}
\label{eq:hprior_norm}
\end{equation}

% ----------------------------------------------------------------------------
\subsection{Relevant expectations}
\subsubsection{Expected variance}
The expected value of $\sigma^2$ is given by,
\begin{equation}
\E{\sigma^2}{h} =
 \int_0^\infty \sigma^2 h(\sigma; \alpha, \beta) \, d\sigma =
 \beta (\sqrt{\alpha} + \beta)
\label{eq:expect_s2}
\end{equation}

\subsubsection{Expected inverse variance}
The expected value of $1 / \sigma^2$ is given by,
\begin{equation}
\E{\frac{1}{\sigma^2}}{h} =
 \int_0^\infty \frac{1}{\sigma^2} h(\sigma; \alpha, \beta) \, d\sigma =
 \frac{1}{\beta \sqrt{\alpha}}
\label{eq:expect_s2_inv}
\end{equation}

\subsubsection{Entropy}
Using \eqref{eq:expect_s2} and \eqref{eq:expect_s2_inv}, the entropy
of the $h\ell_1$ density is found to equal,
\begin{equation}
\mathbb{H}[h(\sigma; \alpha, \beta)] =
 -\E{\ln h(\sigma; \alpha, \beta)}{h} =
 \frac{1}{2} + \frac{1}{2} \ln\left( \frac{\pi \beta^2}{2} \right)
\label{eq:hprior_entropy}
\end{equation}

% ----------------------------------------------------------------------------
\subsection{Relationship to the Laplace density}
The probability density function of a Laplace-distributed random variable
$x \sim \mathcal{L}(\xi)$ is given by,
\begin{equation}
f_{\mathcal{L}}(x; \xi) =
 \frac{1}{2 \xi} \exp\left\{
  -\frac{|x|}{\xi}
 \right\}
\label{eq:laplace_pdf}
\end{equation}
where $\xi > 0$ is called the scale parameter. This density may be
obtained by marginalizing a suitably constructed joint density
involving $x$. More concretely, consider the joint probability
density of the random variables $x$ and $\sigma$, where:
\begin{itemize}
 \item $x \mid \sigma \sim \mathcal{N}(0, \sigma^2)$, i.e.\
  conditioned on $\sigma$, $x$ is normally distributed with
  zero mean and variance $\sigma^2$, and
 \item $\sigma \sim \mathcal{R}(\xi)$, i.e.\
  $\sigma$ is Rayleigh-distributed with scale parameter $\xi$.
\end{itemize}

The joint density over $(x,\sigma)$ is equal to,
\begin{equation}
\begin{aligned}
p(x, \sigma) &=
 p(x \mid \sigma) \, p(\sigma)
\\ &=
 f_{\mathcal{N}}(x; 0, \sigma^2) \,
 f_{\mathcal{R}}(\sigma; \xi)
\\ &=
 \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{
  -\frac{|x|^2}{2 \sigma^2}
 \right\} \cdot
 \frac{\sigma}{\xi^2} \exp\left\{
  -\frac{\sigma^2}{2 \xi^2}
 \right\}
\\ &=
 \frac{1}{\xi^2 \sqrt{2 \pi}} \exp\left\{
  -\frac{|x|^2}{2 \sigma^2}
  -\frac{\sigma^2}{\xi^2}
 \right\}
\end{aligned}
\label{eq:joint_pdf}
\end{equation}
which is proportional to an $h\ell_1$ density having parameters
$\alpha = |x|^2$ and $\beta = \xi$. Marginalizing this joint density
with respect to $\sigma$ using \eqref{eq:hprior_z} results in,
\begin{equation}
\begin{aligned}
p(x) =
\int_0^\infty p(x, \sigma) \, d\sigma &=
 \frac{1}{\xi^2 \sqrt{2 \pi}} \int_0^\infty \exp\left\{
  -\frac{|x|^2}{2 \sigma^2}
  -\frac{\sigma^2}{\xi^2}
 \right\} d\sigma
\\ &=
 \frac{1}{\xi^2 \sqrt{2 \pi}} Z(|x|^2, \xi)
\\ &=
 \frac{1}{\xi^2 \sqrt{2 \pi}}
 \xi \sqrt{\frac{\pi}{2}}
 \exp\left\{ -\frac{\sqrt{|x|^2}}{\xi} \right\}
\\ &=
 \frac{1}{2 \xi} \exp\left\{ -\frac{|x|}{\xi} \right\}
\end{aligned}
\label{eq:marginal_pdf}
\end{equation}
which is a Laplace distribution on $x$ with shape parameter $\xi$.

% ============================================================================
\end{document}
